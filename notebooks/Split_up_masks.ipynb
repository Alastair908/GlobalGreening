{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea74fe56-ffa3-448b-9fee-c9b5adbe399c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf771e3c-5248-4fd3-bfb8-2f6b16e27aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 14:24:11.866415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 14:24:16.348511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-12 14:24:16.349452: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-12 14:24:16.349466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "# from patchify import patchify\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil #, cv2\n",
    "from google.cloud import storage\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb41202-5487-42c9-9f05-9733992b2144",
   "metadata": {},
   "source": [
    "## Key params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e579b0a0-b578-438b-8114-80b6baa6cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DATA_SIZE=15000\n",
    "LAND_USE_ARRAY_SIZE=250\n",
    "TRIAL_SIZE=1000     # after running it once change to a larger number (up to 15000)\n",
    "LOAD_CHUNK_SIZE=TRIAL_SIZE\n",
    "DATA_RUN = 'local_VM' #  select from these options:'gcp' local_VM' , 'local' \n",
    "\n",
    "# on VM workbench in GCP when loading data locally from VM  - change to what is has to be \n",
    "VM_dataset_folder = \"/home/jupyter/GlobalGreening\"  # need to change this \n",
    "VM_images_dir = \"zoomed_photos\"\n",
    "VM_masks_download_dir = 'ESA_worldcover'\n",
    "VM_masks_upload_dir = 'masks'\n",
    "VM_output_folder = \"/home/jupyter/GlobalGreening/training_outputs\" # need to change this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd87eb-c656-4a31-919d-a7f1829dcb28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708d5528-9a7e-443e-8c79-c862b640a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/GlobalGreening\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e7832-bb04-46ba-85d4-f70a363bf171",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions to identify images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4040e8c-b812-47c4-8f74-349ac62d961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used to load images\n",
    "\n",
    "def list_image_filenames(dataset_root_folder, images_dir):\n",
    "    \"\"\"Lists all the files in the images dir.\"\"\"\n",
    "    images_directory = f'{dataset_root_folder}/{images_dir}'\n",
    "    print(f'loading file names from {images_directory}')\n",
    "    file_list = os.listdir(images_directory)\n",
    "    images_file_list = []\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith('.png'):\n",
    "            images_file_list.append(file_name)\n",
    "        \n",
    "    image_files = np.sort(images_file_list)\n",
    "\n",
    "    return image_files\n",
    "\n",
    "def list_mask_filenames(dataset_root_folder, masks_dir):\n",
    "    \n",
    "    \"\"\"Lists all the files in the masks dir.\"\"\"\n",
    "    masks_directory = f'{dataset_root_folder}/{masks_dir}'\n",
    "    print(f'loading file names from {masks_directory}')\n",
    "    file_list = os.listdir(masks_directory)\n",
    "    \n",
    "    masks_file_list = []\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith('.npy'):\n",
    "            masks_file_list.append(file_name)\n",
    "        \n",
    "    mask_files = np.sort(masks_file_list)\n",
    "\n",
    "    return mask_files\n",
    "\n",
    "def get_image_geo_locations(dataset_root_folder, images_dir):\n",
    "    # generating file names from the directory\n",
    "    \n",
    "    image_files = list_image_filenames(dataset_root_folder, images_dir)\n",
    "      \n",
    "    file_names = np.char.rstrip(image_files, '.png')\n",
    "    file_names = np.char.split(file_names, '_') \n",
    "\n",
    "    print(f'sample images file names {image_files[:3]} and amount of images in the directory {len(image_files)}')\n",
    "    print(f'sample split file names {file_names[:3]}, and check that same lenght of lists {len(file_names)}')\n",
    "\n",
    "    # creating df with information about longitude, latitude (can also be used to load the images)\n",
    "    image_geo_locations = np.zeros((len(file_names),2))\n",
    "    image_geo_locations = pd.DataFrame(image_geo_locations, columns=['latitude', 'longitude'])\n",
    "\n",
    "    for image_type in ['latitude', 'longitude']:\n",
    "        for i in range(len(file_names)):\n",
    "            file = file_names[i]\n",
    "            \n",
    "            if image_type == 'latitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                latitude = file[1].strip('-') \n",
    "    #            print(f'latitude is {latitude}')\n",
    "                image_geo_locations.at[int(image_number),'latitude'] = latitude\n",
    "                                    \n",
    "            elif image_type == 'longitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                longitude = file[2] \n",
    "    #            print(f'longitude is {longitude}')\n",
    "                image_geo_locations.at[int(image_number),'longitude'] = longitude                     \n",
    "    return image_geo_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ffc1e-d109-4fef-9672-8fa1f77f5c99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2571417e-c470-458d-a32e-a91d3df23025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file names from /home/jupyter/GlobalGreening/zoomed_photos\n",
      "sample images file names ['image0_-109.0_37.0.png' 'image10000_-104.47_38.54.png'\n",
      " 'image10001_-104.47_38.58.png'] and amount of images in the directory 15415\n",
      "sample split file names [list(['image0', '-109.0', '37.0'])\n",
      " list(['image10000', '-104.47', '38.54'])\n",
      " list(['image10001', '-104.47', '38.58'])], and check that same lenght of lists 15415\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>102.04</td>\n",
       "      <td>40.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15412</th>\n",
       "      <td>102.04</td>\n",
       "      <td>40.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15413</th>\n",
       "      <td>102.04</td>\n",
       "      <td>40.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15414</th>\n",
       "      <td>102.04</td>\n",
       "      <td>40.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15415</th>\n",
       "      <td>102.04</td>\n",
       "      <td>40.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15416 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude longitude\n",
       "0        109.0      37.0\n",
       "1        109.0     37.04\n",
       "2        109.0     37.09\n",
       "3        109.0     37.13\n",
       "4        109.0     37.17\n",
       "...        ...       ...\n",
       "15411   102.04      40.8\n",
       "15412   102.04     40.84\n",
       "15413   102.04     40.89\n",
       "15414   102.04     40.93\n",
       "15415   102.04     40.97\n",
       "\n",
       "[15416 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_geo_locations = get_image_geo_locations(VM_dataset_folder, VM_images_dir)\n",
    "image_geo_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fd021e0-f85e-453e-931d-2c2193fc0842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15416"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_geo_locations.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd85e95d-3577-4887-bbac-20d396a15a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15416"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_geo_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47415912-af9f-4029-a52c-13151a78a811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2df689a5-f29b-45cb-813c-c56b55bec7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file names from /home/jupyter/GlobalGreening/ESA_worldcover\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_mask_files = list_mask_filenames(VM_dataset_folder, VM_masks_dir)\n",
    "len(mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46f5b60f-60dc-4833-8995-10eb28e01c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file names from /home/jupyter/GlobalGreening/zoomed_photos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['image0_-109.0_37.0.png', 'image10000_-104.47_38.54.png',\n",
       "       'image10001_-104.47_38.58.png'], dtype='<U28')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_files = list_image_filenames(VM_dataset_folder, VM_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "534e9c8b-610a-4504-a704-8274ebfa7921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15415"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e32bd30-272a-44f4-8325-03699c33109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image15400_-102.04_40.33.png\n",
      "image15401_-102.04_40.37.png\n",
      "image15402_-102.04_40.42.png\n",
      "image15403_-102.04_40.46.png\n",
      "image15404_-102.04_40.5.png\n",
      "image15405_-102.04_40.54.png\n",
      "image15406_-102.04_40.59.png\n",
      "image15407_-102.04_40.63.png\n",
      "image15408_-102.04_40.67.png\n",
      "image15409_-102.04_40.71.png\n",
      "image1540_-108.32_38.54.png\n",
      "image15410_-102.04_40.76.png\n",
      "image15411_-102.04_40.8.png\n",
      "image15412_-102.04_40.84.png\n",
      "image15413_-102.04_40.89.png\n",
      "image15414_-102.04_40.93.png\n",
      "image15415_-102.04_40.97.png\n",
      "image1541_-108.32_38.58.png\n",
      "image1542_-108.32_38.62.png\n",
      "image1543_-108.32_38.67.png\n",
      "image1544_-108.32_38.71.png\n",
      "image1545_-108.32_38.75.png\n",
      "image1546_-108.32_38.79.png\n",
      "image1547_-108.32_38.84.png\n",
      "image1548_-108.32_38.88.png\n",
      "image1549_-108.32_38.92.png\n",
      "image154_-108.96_39.56.png\n"
     ]
    }
   ],
   "source": [
    "for image in image_files:\n",
    "    if \"image154\" in image:\n",
    "        print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08120329-4043-4962-a00b-53b595d67589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5827ea8-21ca-4135-b382-639f216d85a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15416,\n",
       " ['mask15406_-102.04_40.59.npy',\n",
       "  'mask15407_-102.04_40.63.npy',\n",
       "  'mask15408_-102.04_40.67.npy',\n",
       "  'mask15409_-102.04_40.71.npy',\n",
       "  'mask15410_-102.04_40.76.npy',\n",
       "  'mask15411_-102.04_40.8.npy',\n",
       "  'mask15412_-102.04_40.84.npy',\n",
       "  'mask15413_-102.04_40.89.npy',\n",
       "  'mask15414_-102.04_40.93.npy',\n",
       "  'mask15415_-102.04_40.97.npy'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_upload_file_names = []\n",
    "\n",
    "for i in range(len(image_files)):\n",
    "    mask_file =f'mask{i}_-{image_geo_locations.iat[i,0]}_{image_geo_locations.iat[i,1]}.npy'\n",
    "    mask_upload_file_names.append(mask_file)\n",
    "\n",
    "len(mask_upload_file_names), mask_upload_file_names[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f5637-629e-4783-b66b-f61456b516a4",
   "metadata": {},
   "source": [
    "### Function to change numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f14e6e0-c83e-4d30-bad1-6a43ad072d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mask_files = len(download_mask_files)\n",
    "total_mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "937f45d2-da1b-44be-a4ed-14a43fa81ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['land_use_data_from_0_to_249.npy'], dtype='<U37')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_mask_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bddfe318-6672-4088-be6e-502d415250ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the masks from these file land_use_data_from_0_to_249.npy\n",
      "loading array land_use_data_from_0_to_249.npy into mask dataset with shape (250, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# load one numpy of 250 masks into masks_dataset\n",
    "\n",
    "for mask_file in download_mask_files[:1]:\n",
    "    print(f'loading the masks from these file {mask_file}')\n",
    "    path_mask = f'{VM_dataset_folder}/{VM_masks_download_dir}/{mask_file}'    \n",
    "    \n",
    "    masks_dataset = np.load(path_mask)\n",
    "    print(f'loading array {mask_file} into mask dataset with shape {masks_dataset.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1ab938e-a7f1-4a49-8779-42c24bd6ca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 250, (250, 512, 512, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(masks_dataset), len(masks_dataset), masks_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "896ad506-03e4-4543-ab5e-8465ec7cee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range is 15250, 15415\n",
      "number of files is 166\n"
     ]
    }
   ],
   "source": [
    "# testing range\n",
    "i=61\n",
    "from_masks_array_file = i*LAND_USE_ARRAY_SIZE\n",
    "to_masks_array_file = min((i+1)*LAND_USE_ARRAY_SIZE-1, 15415)\n",
    "range_mask_uploading = to_masks_array_file - from_masks_array_file + 1\n",
    "\n",
    "\n",
    "print(f'range is {from_masks_array_file}, {to_masks_array_file}')\n",
    "print(f'number of files is {range_mask_uploading}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ac02afa-3c9f-48b7-8bba-6b69ac4aa73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask500_-108.79_38.28.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask500_-108.79_38.28.npy\n",
      "mask501_-108.79_38.32.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask501_-108.79_38.32.npy\n",
      "mask502_-108.79_38.37.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask502_-108.79_38.37.npy\n",
      "mask503_-108.79_38.41.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask503_-108.79_38.41.npy\n",
      "mask504_-108.79_38.45.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask504_-108.79_38.45.npy\n",
      "mask505_-108.79_38.49.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask505_-108.79_38.49.npy\n",
      "mask506_-108.79_38.54.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask506_-108.79_38.54.npy\n",
      "mask507_-108.79_38.58.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask507_-108.79_38.58.npy\n",
      "mask508_-108.79_38.62.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask508_-108.79_38.62.npy\n",
      "mask509_-108.79_38.67.npy\n",
      "/home/jupyter/GlobalGreening/masks/mask509_-108.79_38.67.npy\n"
     ]
    }
   ],
   "source": [
    "## testing code to save into new files\n",
    "\n",
    "for j in range(from_masks_array_file,from_masks_array_file+10):\n",
    "        mask_upload_file =f'mask{j}_-{image_geo_locations.iat[j,0]}_{image_geo_locations.iat[j,1]}.npy'\n",
    "        print(mask_upload_file)\n",
    "        path_upload_mask = f'{VM_dataset_folder}/{VM_masks_upload_dir}/{mask_upload_file}'\n",
    "        print(path_upload_mask)\n",
    "        #np.save(path_upload_mask, masks_dataset[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "845e48d9-2110-4a35-b6c5-f6e5751dd8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_9750_to_9999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_9750_to_9999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10000_to_10249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10000_to_10249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10250_to_10499.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10250_to_10499.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10500_to_10749.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10500_to_10749.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10750_to_10999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_10750_to_10999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11000_to_11249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11000_to_11249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11250_to_11499.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11250_to_11499.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11500_to_11749.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11500_to_11749.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11750_to_11999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_11750_to_11999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12000_to_12249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12000_to_12249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12250_to_12499.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12250_to_12499.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12500_to_12749.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12500_to_12749.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12750_to_12999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_12750_to_12999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13000_to_13249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13000_to_13249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13250_to_13499.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13250_to_13499.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13500_to_13749.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13500_to_13749.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13750_to_13999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_13750_to_13999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14000_to_14249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14000_to_14249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14250_to_14499.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14250_to_14499.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14500_to_14749.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14500_to_14749.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14750_to_14999.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_14750_to_14999.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_15000_to_15249.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_15000_to_15249.npy\n",
      "loading the masks from these file /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_15250_to_15415.npy\n",
      "finished another batch /home/jupyter/GlobalGreening/ESA_worldcover/land_use_data_from_15250_to_15415.npy\n"
     ]
    }
   ],
   "source": [
    "### loading each numpy array and then savings the files\n",
    "### save each part of numpy array as separate file\n",
    "\n",
    "LAND_USE_ARRAY_SIZE=250\n",
    "\n",
    "for i in range(39,62): #i create numpy array file names for masks (e..g 0-249, 250-499, 15250-15415 (should be 15415)\n",
    "    from_masks_array_file = i*LAND_USE_ARRAY_SIZE\n",
    "    to_masks_array_file = min((i+1)*LAND_USE_ARRAY_SIZE-1 , 15415)\n",
    "    \n",
    "    mask_download_file = f'land_use_data_from_{from_masks_array_file}_to_{to_masks_array_file}.npy'\n",
    "    \n",
    "    path_download_mask = f'{VM_dataset_folder}/{VM_masks_download_dir}/{mask_download_file}'\n",
    "    \n",
    "    print(f'loading the masks from these file {path_download_mask}')\n",
    "        \n",
    "    masks_dataset = np.load(path_download_mask)\n",
    "\n",
    "    range_mask_uploading = to_masks_array_file - from_masks_array_file + 1\n",
    "    \n",
    "    for j in range(range_mask_uploading):\n",
    "#        print(f'range is {from_masks_array_file} - {to_masks_array_file}')\n",
    "        mask_file_loc = (i*LAND_USE_ARRAY_SIZE) + j\n",
    "        mask_upload_file =f'mask{mask_file_loc}_-{image_geo_locations.iat[mask_file_loc,0]}_{image_geo_locations.iat[mask_file_loc,1]}.npy'\n",
    "        path_upload_mask = f'{VM_dataset_folder}/{VM_masks_upload_dir}/{mask_upload_file}'\n",
    "        np.save(path_upload_mask, masks_dataset[j])\n",
    "    \n",
    "    print(f'finished another batch {path_download_mask}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f71b579-af48-4958-83ce-0731bdc70c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file names from /home/jupyter/GlobalGreening/masks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15416"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing if it worked\n",
    "total_uploaded_mask_files = list_mask_filenames(VM_dataset_folder, VM_masks_upload_dir)\n",
    "len(total_uploaded_mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "63c409fc-cc0a-452b-9eb5-73969f337c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file names from /home/jupyter/GlobalGreening/masks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_mask_files = list_mask_filenames(VM_dataset_folder,VM_masks_upload_dir)\n",
    "len(upload_mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed9e64-dd61-442a-9812-3e69cb0221c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8799b53-98ce-4f0d-a7f7-ce719713adbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b6e81-53c4-40ae-8466-a7f6f55c6066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a884f040-17a7-4db7-814a-7f0674ad8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_range = 0\n",
    "stop_range = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c067ce6c-1910-437b-9ef6-d69ec286e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load range for images is 1000\n",
      "load range for masks is 4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba76176-faac-4724-ae28-29b1db2f0c76",
   "metadata": {},
   "source": [
    "### Functions not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5468bec-6697-41eb-bec1-0c88d18a680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masks(dataset_root_folder, masks_dir, load_range_masks, land_use_array_size):\n",
    "\n",
    "    for i in range(load_range_masks):\n",
    "        mask_file = f'land_use_data_from_{i*land_use_array_size}_to_{(i+1)*land_use_array_size-1}.npy'\n",
    "        path_mask = f'{dataset_root_folder}/{masks_dir}/{mask_file}'\n",
    "#        print(f'loading the masks from these file {path_mask}')\n",
    "        \n",
    "        if i == 0:\n",
    "            masks_dataset = np.load(path_mask)\n",
    "#            print(f'loading array {i+1} into mask dataset with shape {masks_dataset.shape}')\n",
    "    \n",
    "        else:\n",
    "            array_to_append = np.load(path_mask)\n",
    "            masks_dataset = np.vstack((masks_dataset, array_to_append))\n",
    "#            print(f'appending to masks_dataset an array {i+1} with shape {array_to_append.shape}')\n",
    "    \n",
    "    return masks_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2819f-9572-4525-ab6f-51eb4fee0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(dataset_root_folder, images_dir, load_range_images):\n",
    "    \n",
    "    image_geo_locations = get_image_geo_locations(dataset_root_folder, images_dir)\n",
    "\n",
    "    # loading images into the list (new code so the files are loaded in correct order (by index))\n",
    "    images_dataset = []\n",
    "\n",
    "    for i in range(load_range_images):\n",
    "        image_file =f'image{i}_-{image_geo_locations.iat[i,0]}_{image_geo_locations.iat[i,1]}.png'\n",
    "        path_image = f'{dataset_root_folder}/{images_dir}/{image_file}'\n",
    "#        print(f'loading the image from these file {path_image}')\n",
    "        \n",
    "        image = Image.open(path_image)\n",
    "        type(image)\n",
    "        if np.asarray(image).shape[2] >3: \n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        images_dataset.append(image)\n",
    "#        print(f'appended image of size {image.shape}')\n",
    "    \n",
    "    images_dataset = np.array(images_dataset)\n",
    "    \n",
    "    return images_dataset\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
