{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfc6910",
   "metadata": {},
   "source": [
    "# Global Greening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48798",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Installing & Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d40ec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "this notebook version is setup so we can run the model based on data locally on VM machine in GCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ac69a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "# from patchify import patchify\n",
    "# import albumentations as A\n",
    "from IPython.display import SVG\n",
    "#import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil #, cv2\n",
    "from google.cloud import storage\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64952188",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a3108",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[key link](https://github.com/kunnalparihar/Satellite-Image-Segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d1134",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepara Data Augmentation - ignore, not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d5c7a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Augmentation using Albumentations Library**\n",
    "\n",
    "[Albumentations](https://albumentations.ai/) is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n",
    "\n",
    "Data augmentation is done by the following techniques:\n",
    "\n",
    "1. Random Cropping - left out since we will have same size pictures\n",
    "2. Horizontal Flipping\n",
    "3. Vertical Flipping\n",
    "4. Rotation\n",
    "5. Random Brightness & Contrast\n",
    "6. Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "7. Grid Distortion\n",
    "8. Optical Distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185561c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # function to augment\n",
    "# def augment(): #width, height\n",
    "#     transform = A.Compose([\n",
    "# #        A.RandomCrop(width=width, height=height, p=1.0),\n",
    "#         A.HorizontalFlip(p=1.0),\n",
    "#         A.VerticalFlip(p=1.0),\n",
    "#         A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n",
    "#         A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n",
    "#         A.OneOf([\n",
    "#             A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),\n",
    "#             A.GridDistortion(p=0.5),\n",
    "#             A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n",
    "#         ], p=1.0),\n",
    "#     ], p=1.0)\n",
    "    \n",
    "#     return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e35f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # visualize the augmentations\n",
    "\n",
    "# def visualize(image, mask, original_image=None, original_mask=None):\n",
    "#     fontsize = 16\n",
    "\n",
    "#     if original_image is None and original_mask is None:\n",
    "#         f, ax = plt.subplots(2, 1, figsize=(10, 10)) \n",
    "\n",
    "#         ax[0].imshow(image)\n",
    "#         ax[1].imshow(mask)\n",
    "#     else:\n",
    "#         f, ax = plt.subplots(2, 2, figsize=(16, 12))  \n",
    "\n",
    "#         ax[0, 0].imshow(original_image)\n",
    "#         ax[0, 0].set_title('Original Image', fontsize=fontsize)\n",
    "\n",
    "#         ax[1, 0].imshow(original_mask)\n",
    "#         ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n",
    "\n",
    "#         ax[0, 1].imshow(image)\n",
    "#         ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n",
    "\n",
    "#         ax[1, 1].imshow(mask)\n",
    "#         ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n",
    "        \n",
    "#     plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec164c77",
   "metadata": {},
   "source": [
    "## Key Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536f7a0",
   "metadata": {},
   "source": [
    "Please make sure to create local folder structure in following way:\n",
    "\n",
    "     |\n",
    "     |---- raw_data\n",
    "     |    |---- zoomed_photos\n",
    "     |    |---- ESA_worldcover\n",
    "     |\n",
    "     |---- training_outputs\n",
    "     |    |---- metrics\n",
    "     |    |---- models\n",
    "     |    |---- params\n",
    "\n",
    "there will a folder for predictions as well, but will sort it out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac47efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DATA_SIZE=1500\n",
    "LOAD_CHUNK_SIZE=500\n",
    "LAND_USE_ARRAY_SIZE=250\n",
    "TRIAL_SIZE=1000     # after running it once change to a larger number (up to 15000)\n",
    "DATA_RUN = 'local_VM' #  select from these options:'gcp' local_VM' , 'local' \n",
    "\n",
    "# on Tim's GCP loading data from Bucket\n",
    "BUCKET_NAME = \"GlobalGreening\"\n",
    "images_dir = \"zoomed_photos\"\n",
    "masks_dir = \"ESA_worldcover\"\n",
    "training_output_dir = \"training_outputs\"\n",
    "\n",
    "# on VM workbench in GCP when loading data locally from VM  - change to what is has to be \n",
    "VM_dataset_folder = \"/home/jupyter/GlobalGreening\"  # need to change this \n",
    "VM_images_dir = \"zoomed_photos\"\n",
    "VM_masks_dir = 'ESA_worldcover'\n",
    "VM_output_folder = \"/home/jupyter/GlobalGreening/training_outputs\" # need to change this \n",
    "\n",
    "### for local use - Alenka's computer\n",
    "local_dataset_folder = \"/Users/Alenka/code/Alastair908/GlobalGreening/raw_data\"\n",
    "local_images_dir = \"images_trial_run\"\n",
    "local_masks_dir = \"masks_trial_run\"\n",
    "local_output_folder = \"/Users/Alenka/code/Alastair908/GlobalGreening/training_outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to store model outputs\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if DATA_RUN == 'gcp':\n",
    "    pass # not completed yet\n",
    "\n",
    "elif DATA_RUN == 'local_VM':\n",
    "    model_path = os.path.join(VM_output_folder, \"models\", f\"{timestamp}_InceptionResNetV2-UNet.h5\")\n",
    "    results_path = os.path.join(VM_output_folder, \"metrics\", f\"{timestamp}_model_training.csv\")\n",
    "    model_metrics_plot_path = os.path.join(VM_output_folder, \"metrics\", f\"{timestamp}_model_metrics_plot.png\")\n",
    "    predictions_dir = os.path.join(VM_output_folder, f\"{timestamp}_predictions\")\n",
    "    \n",
    "elif DATA_RUN == 'local':\n",
    "    model_path = os.path.join(local_output_folder, \"models\", f\"{timestamp}_InceptionResNetV2-UNet.h5\")\n",
    "    results_path = os.path.join(local_output_folder, \"metrics\", f\"{timestamp}_model_training.csv\")\n",
    "    model_metrics_plot_path = os.path.join(local_output_folder, \"metrics\", f\"{timestamp}_model_metrics_plot.png\")\n",
    "    predictions_dir = os.path.join(local_output_folder,  f\"{timestamp}_predictions\")\n",
    "    \n",
    "print(\"--------\")\n",
    "print(f' model path is {model_path}')\n",
    "print(\"--------\")\n",
    "print(f' results path is {results_path}')\n",
    "print(\"--------\")\n",
    "print(f' model metrics path is {model_metrics_plot_path}')\n",
    "print(\"--------\")\n",
    "print(f'predictions folder is {predictions_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** NOT USED YET (folder to save predictions) ****** \n",
    "\n",
    "# # please make predictions directory based on what was printed out\n",
    "# # check if any folders need to be created and adjust the below command \n",
    "# (see pwd path above)\n",
    "\n",
    "# !mkdir -p ../training_outputs/20230610-132230_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b841e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d705dd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Functions to load data locally or locally from VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e0f7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# functions used to load images\n",
    "\n",
    "def list_image_filenames(dataset_root_folder, images_dir):\n",
    "    \"\"\"Lists all the files in the images dir.\"\"\"\n",
    "    images_directory = f'{dataset_root_folder}/{images_dir}'\n",
    "    print(f'loading file names from {images_directory}')\n",
    "    file_list = os.listdir(images_directory)\n",
    "    \n",
    "    image_file_list = []\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith('.png'):\n",
    "            image_file_list.append(file_name)\n",
    "        \n",
    "    image_files = np.sort(image_file_list)\n",
    "\n",
    "    return image_files\n",
    "\n",
    "def get_image_geo_locations(dataset_root_folder, images_dir):\n",
    "    # generating file names from the directory\n",
    "    \n",
    "    image_files = list_image_filenames(dataset_root_folder, images_dir)\n",
    "      \n",
    "    file_names = np.char.rstrip(image_files, '.png')\n",
    "    file_names = np.char.split(file_names, '_') \n",
    "\n",
    "    print(f'sample images file names {image_files[:3]} and amount of images in the directory {len(image_files)}')\n",
    "    print(f'sample split file names {file_names[:3]}, and check that same lenght of lists {len(file_names)}')\n",
    "\n",
    "    # creating df with information about longitude, latitude (can also be used to load the images)\n",
    "    image_geo_locations = np.zeros((len(file_names),2))\n",
    "    image_geo_locations = pd.DataFrame(image_geo_locations, columns=['latitude', 'longitude'])\n",
    "\n",
    "    for image_type in ['latitude', 'longitude']:\n",
    "        for i in range(len(file_names)):\n",
    "            file = file_names[i]\n",
    "            \n",
    "            if image_type == 'latitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                latitude = file[1].strip('-') \n",
    "    #            print(f'latitude is {latitude}')\n",
    "                image_geo_locations.at[int(image_number),'latitude'] = latitude\n",
    "                                    \n",
    "            elif image_type == 'longitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                longitude = file[2] \n",
    "    #            print(f'longitude is {longitude}')\n",
    "                image_geo_locations.at[int(image_number),'longitude'] = longitude                     \n",
    "    return image_geo_locations\n",
    "\n",
    "def load_images(dataset_root_folder, images_dir, load_range_images):\n",
    "    \n",
    "    image_geo_locations = get_image_geo_locations(dataset_root_folder, images_dir)\n",
    "\n",
    "    # loading images into the list (new code so the files are loaded in correct order (by index))\n",
    "    images_dataset = []\n",
    "\n",
    "    for i in range(load_range_images):\n",
    "        image_file =f'image{i}_-{image_geo_locations.iat[i,0]}_{image_geo_locations.iat[i,1]}.png'\n",
    "        path_image = f'{dataset_root_folder}/{images_dir}/{image_file}'\n",
    "#        print(f'loading the image from these file {path_image}')\n",
    "        \n",
    "        image = Image.open(path_image)\n",
    "        type(image)\n",
    "        if np.asarray(image).shape[2] >3: \n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        images_dataset.append(image)\n",
    "#        print(f'appended image of size {image.shape}')\n",
    "    \n",
    "    images_dataset = np.array(images_dataset)\n",
    "    \n",
    "    return images_dataset\n",
    "\n",
    "\n",
    "def load_masks(dataset_root_folder, masks_dir, load_range_masks, land_use_array_size):\n",
    "\n",
    "    for i in range(load_range_masks):\n",
    "        mask_file = f'land_use_data_from_{i*land_use_array_size}_to_{(i+1)*land_use_array_size-1}.npy'\n",
    "        path_mask = f'{dataset_root_folder}/{masks_dir}/{mask_file}'\n",
    "#        print(f'loading the masks from these file {path_mask}')\n",
    "        \n",
    "        masks_dataset_dir = f'masks_dataset{i+1}'\n",
    "        if i == 0:\n",
    "            masks_dataset = np.load(path_mask)\n",
    "#            print(f'loading array {i+1} into mask dataset with shape {masks_dataset.shape}')\n",
    "    \n",
    "        else:\n",
    "            array_to_append = np.load(path_mask)\n",
    "            masks_dataset = np.vstack((masks_dataset, array_to_append))\n",
    "#            print(f'appending to masks_dataset an array {i+1} with shape {array_to_append.shape}')\n",
    "    \n",
    "    return masks_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55535f35",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Functions to load data from bucket (GCP option) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b48e91",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # some options to open files from Bucket/blobs directly - \n",
    "# # not used as I am not sure it works with GCP\n",
    "\n",
    "# def open_image_from_bucket(bucket, blob_file_path):\n",
    "#     storage_client = storage.Client()\n",
    "#     file_byte_string = storage_client.get_object(Bucket=bucket, Key=key)['Body'].read()\n",
    "#     return Image.open(BytesIO(file_byte_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a2d07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# list files in blobs\n",
    "\n",
    "def list_blobs(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    image_file_list = []\n",
    "    mask_file_list = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        file_name = blob.split('/')[1]\n",
    "        \n",
    "        if file_name.endswith('.png'):\n",
    "            image_file_list.append(file_name)\n",
    "        if file_name.endswith('.npy'):\n",
    "            mask_file_list.append(file_name)\n",
    "        \n",
    "    image_file_list = np.sort(image_file_list)\n",
    "    mask_file_list = np.sort(mask_file_list)\n",
    "\n",
    "    return image_file_list, mask_file_list\n",
    "\n",
    "\n",
    "# functions used to load images from buckets, and from blobs\n",
    "\n",
    "def bucket_get_image_geo_locations(bucket_name):\n",
    "    # generating file names from the directory\n",
    "    print(f'loading image and mask file names from {bucket_name}')\n",
    "    image_file_list, mask_file_list = list_blobs(bucket_name)\n",
    "\n",
    "    file_names = np.char.rstrip(image_file_list, '.png')\n",
    "    file_names = np.char.split(file_names, '_') \n",
    "\n",
    "    print(f'sample images file names {image_file_list[:3]} and amount of images in the directory {len(image_file_list)}')\n",
    "    print(f'sample split file names {file_names[:3]}, and check that same lenght of lists {len(file_names)}')\n",
    "\n",
    "    # creating df with information about longitude, latitude (can also be used to load the images)\n",
    "    image_geo_locations = np.zeros((len(file_names),2))\n",
    "    image_geo_locations = pd.DataFrame(image_geo_locations, columns=['latitude', 'longitude'])\n",
    "\n",
    "    for image_type in ['latitude', 'longitude']:\n",
    "        for i in range(len(file_names)):\n",
    "            file = file_names[i]\n",
    "            \n",
    "            if image_type == 'latitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                latitude = file[1].strip('-') \n",
    "    #            print(f'latitude is {latitude}')\n",
    "                image_geo_locations.at[int(image_number),'latitude'] = latitude\n",
    "                                    \n",
    "            elif image_type == 'longitude':\n",
    "                text = file[0]\n",
    "                image_number = ''.join(num for num in text if num.isdigit())\n",
    "                longitude = file[2] \n",
    "    #            print(f'longitude is {longitude}')\n",
    "                image_geo_locations.at[int(image_number),'longitude'] = longitude                     \n",
    "    return image_geo_locations\n",
    "\n",
    "def bucket_load_images(bucket_name, images_dir, load_range_images):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    image_geo_locations = get_image_geo_locations(bucket_name)\n",
    "    image_file_list, mask_file_list = list_blobs(bucket_name)\n",
    "    \n",
    "    # loading images into the list (new code so the files are loaded in correct order (by index))\n",
    "    \n",
    "    images_dataset = []\n",
    "\n",
    "    for i in range(load_range_images):\n",
    "        image_file =f'image{i}_-{image_geo_locations.iat[i,0]}_{image_geo_locations.iat[i,1]}.png'\n",
    "        path_image = f'{images_dir}/{image_file}'\n",
    "        print(f'loading the image from these file {path_image}')\n",
    "        \n",
    "        blob = bucket.blob(path_image)\n",
    "        blob.download_to_file(file_obj)\n",
    "        image = Image.open(file_obj)\n",
    "        \n",
    "        if np.asarray(image).shape[2] >3: \n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        images_dataset.append(image)\n",
    "        print(f'appended image of size {image.shape}')\n",
    "    \n",
    "    images_dataset = np.array(images_dataset)\n",
    "    \n",
    "    return images_dataset\n",
    "\n",
    "\n",
    "def bucket_load_masks(bucket_name, masks_dir, load_range_masks, land_use_array_size):\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name) \n",
    "\n",
    "    for i in range(load_range_masks):\n",
    "        mask_file = f'land_use_data_from_{i*land_use_array_size}_to_{(i+1)*land_use_array_size-1}.npy'\n",
    "        path_mask = f'{masks_dir}/{mask_file}'\n",
    "        print(f'loading the masks from these file {path_mask}')\n",
    "\n",
    "        blob = bucket.blob(path_mask)\n",
    "        blob.download_to_file(file_obj)\n",
    "        \n",
    "        masks_dataset_dir = f'masks_dataset{i+1}'\n",
    "        \n",
    "        if i == 0:\n",
    "            masks_dataset = np.load(file_obj)\n",
    "            print(f'loading array {i+1} into mask dataset with shape {masks_dataset.shape}')\n",
    "    \n",
    "        else:\n",
    "            array_to_append = np.load(file_obj)\n",
    "            masks_dataset = np.vstack((masks_dataset, array_to_append))\n",
    "            print(f'appending to masks_dataset an array {i+1} with shape {array_to_append.shape}')\n",
    "    \n",
    "    return masks_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29d492",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Start loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ee3fa-7c8b-4682-b4e5-74263236baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = get_image_geo_locations(VM_dataset_folder, VM_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e940402-2bdd-47a5-9fd5-e1b81265696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dff98",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate some variables that are used for loading\n",
    "\n",
    "# flexible in case we will need LOAD_CHUNK_SIZE later when we \n",
    "# e.g. load 500 images for model, and then next 500 ... \n",
    "load_range_images = min(TRIAL_SIZE, LOAD_CHUNK_SIZE) \n",
    "print(f'load range for images is {load_range_images}')\n",
    "\n",
    "# load_range_masks is number of arrays to load (each one 250 masks) \n",
    "load_range_masks = math.ceil(TRIAL_SIZE/LAND_USE_ARRAY_SIZE) \n",
    "print(f'load range for masks is {load_range_masks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df291ba3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if DATA_RUN == 'gcp':\n",
    "    images_dataset = load_images(BUCKET_NAME, \n",
    "                                 images_dir, \n",
    "                                 TRIAL_SIZE)\n",
    "    masks_dataset = load_masks(BUCKET_NAME, \n",
    "                                masks_dir, \n",
    "                                TRIAL_SIZE, \n",
    "                                LAND_USE_ARRAY_SIZE)\n",
    "\n",
    "elif DATA_RUN == 'local_VM':\n",
    "    images_dataset = load_images(VM_dataset_folder, \n",
    "                                 VM_images_dir, \n",
    "                                 load_range_images)\n",
    "    masks_dataset = load_masks(VM_dataset_folder, \n",
    "                                VM_masks_dir, \n",
    "                                load_range_masks,\n",
    "                                LAND_USE_ARRAY_SIZE)\n",
    "\n",
    "elif DATA_RUN == 'local':\n",
    "    images_dataset = load_images(local_dataset_folder, \n",
    "                                 local_images_dir, \n",
    "                                 load_range_images)\n",
    "    masks_dataset = load_masks(local_dataset_folder, \n",
    "                               local_masks_dir, \n",
    "                               load_range_masks, \n",
    "                               LAND_USE_ARRAY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789705a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Checking the size and type of loaded images, masks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658df14",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run all these cells just to see nothing weird happened while loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8738e4a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(images_dataset), images_dataset[0].shape, type(images_dataset[0]), type(images_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fd571",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(masks_dataset), masks_dataset[0].shape, type(masks_dataset[0]), type(masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d7bc2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_sizes = []\n",
    "\n",
    "for i in range(len(images_dataset)):\n",
    "    image_sizes.append(images_dataset[i].shape)\n",
    "\n",
    "min(image_sizes), max(image_sizes)\n",
    "min(image_sizes), max(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9cd72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask_sizes = []\n",
    "\n",
    "for i in range(len(masks_dataset)):\n",
    "    mask_sizes.append(masks_dataset[i].shape)\n",
    "\n",
    "min(mask_sizes), max(mask_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118746af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check random image + mask combinations\n",
    "\n",
    "i = random.randint(0, TRIAL_SIZE)\n",
    "f, ax = plt.subplots(1, 2, figsize=(6, 6)) \n",
    "ax[0].imshow(images_dataset[i])\n",
    "ax[1].imshow(masks_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42654f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random check of 6 masks and respective images\n",
    "\n",
    "i = random.randint(0, TRIAL_SIZE-5)\n",
    "\n",
    "f, ax = plt.subplots(2, 6, figsize=(15, 6)) \n",
    "\n",
    "ax[0,0].imshow(masks_dataset[i])\n",
    "ax[0,1].imshow(masks_dataset[i+1])\n",
    "ax[0,2].imshow(masks_dataset[i+2])\n",
    "ax[0,3].imshow(masks_dataset[i+3])\n",
    "ax[0,4].imshow(masks_dataset[i+4])\n",
    "ax[0,5].imshow(masks_dataset[i+5])\n",
    "\n",
    "\n",
    "ax[1,0].imshow(images_dataset[i])\n",
    "ax[1,1].imshow(images_dataset[i+1])\n",
    "ax[1,2].imshow(images_dataset[i+2])\n",
    "ax[1,3].imshow(images_dataset[i+3])\n",
    "ax[1,4].imshow(images_dataset[i+4])\n",
    "ax[1,5].imshow(images_dataset[i+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3b92c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Image masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084d013",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Value\tColor\tDescription\n",
    "- 10\t#006400\tTree cover \n",
    "<p align=\"center\"><div style=\"background-color: #006400; padding: 10px; \"/></p>   \n",
    "- 20\t#ffbb22\tShrubland \n",
    "<p align=\"center\"><div style=\"background-color: #ffbb22; padding: 10px; \"/></p>   \n",
    "- 30\t#ffff4c\tGrassland\n",
    "<p align=\"center\"><div style=\"background-color: #ffff4c; padding: 10px; \"/></p>   \n",
    "- 40\t#f096ff\tCropland\n",
    "<p align=\"center\"><div style=\"background-color: #f096ff; padding: 10px; \"/></p>  \n",
    "- 50\t#fa0000\tBuilt-up\n",
    "<p align=\"center\"><div style=\"background-color: #fa0000; padding: 10px; \"/></p>  \n",
    "- 60\t#b4b4b4\tBare / sparse vegetation\n",
    "<p align=\"center\"><div style=\"background-color: #b4b4b4; padding: 10px; \"/></p>  \n",
    "- 70\t#f0f0f0\tSnow and ice\n",
    "<p align=\"center\"><div style=\"background-color: #f0f0f0; padding: 10px; \"/></p>  \n",
    "- 80\t#0064c8\tPermanent water bodies\n",
    "<p align=\"center\"><div style=\"background-color: #0064c8; padding: 10px; \"/></p>  \n",
    "- 90\t#0096a0\tHerbaceous wetland\n",
    "<p align=\"center\"><div style=\"background-color: #0096a0; padding: 10px; \"/></p>  \n",
    "- 95\t#00cf75\tMangroves\n",
    "<p align=\"center\"><div style=\"background-color: #00cf75; padding: 10px; \"/></p>  \n",
    "- 100\t#fae6a0\tMoss and lichen\n",
    "<p align=\"center\"><div style=\"background-color: #fae6a0; padding: 10px; \"/></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94020d40",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Preparing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3a100",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels_dict = {\"classes\": [\n",
    "{\"title\": \"Tree cover\", \"r\": 0 , \"g\": 0 , \"b\": 0 }, \n",
    "{\"title\": \"Shrubland\", \"r\": 0, \"g\": 0, \"b\": 0 }, \n",
    "{\"title\": \"Grassland\", \"r\": 0, \"g\": 0, \"b\": 0 }, \n",
    "{\"title\": \"Cropland\", \"r\": 0, \"g\": 0, \"b\": 0 }, \n",
    "{\"title\": \"Built-up\", \"r\": 0, \"g\": 0, \"b\": 0 }, \n",
    "{\"title\": \"Bare, sparse vegetation\", \"r\": 0, \"g\": 0, \"b\": 0 },\n",
    "{\"title\": \"Snow and ice\", \"r\": 0, \"g\": 0, \"b\": 0 },\n",
    "{\"title\": \"Permanent water bodies\", \"r\": 0, \"g\": 0, \"b\": 0 },\n",
    "{\"title\": \"Herbaceous wetland\", \"r\": 0, \"g\": 0, \"b\": 0 },\n",
    "{\"title\": \"Mangroves\", \"r\": 0, \"g\": 0, \"b\": 0 },\n",
    "{\"title\": \"Moss and lichen\", \"r\": 0, \"g\": 0, \"b\": 0 }\n",
    "]}\n",
    "\n",
    "\n",
    "# loading correct rgb values from hex_color list based on ESA  \n",
    "hex_colors_list = ['#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000',\n",
    "                    '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75', '#fae6a0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f0f1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preparing label df with all information: classes, rgb values\n",
    "\n",
    "def prepare_labels(labels_dict, hex_colors_list):\n",
    "\n",
    "    labels_dict_df = pd.DataFrame(labels_dict['classes'])\n",
    "    \n",
    "    for i in range(len(hex_colors_list)):\n",
    "        color = hex_colors_list[i].lstrip('#')\n",
    "        r = int(color[0:2],16)\n",
    "        g = int(color[2:4],16)\n",
    "        b = int(color[4:6],16)\n",
    "        labels_dict_df.at[i,'r'] = r\n",
    "        labels_dict_df.at[i,'g'] = g\n",
    "        labels_dict_df.at[i,'b'] = b\n",
    "    \n",
    "    return labels_dict_df\n",
    "\n",
    "# preparing label codes (just rgb values in order of classes)\n",
    "\n",
    "def prepare_label_codes(labels_dict, hex_colors_list):\n",
    "    labels_dict_df = prepare_labels(labels_dict, hex_colors_list)\n",
    "    \n",
    "    label_codes = []\n",
    "    r= np.asarray(labels_dict_df.r)\n",
    "    g= np.asarray(labels_dict_df.g)\n",
    "    b= np.asarray(labels_dict_df.b)\n",
    "\n",
    "    for i in range(len(labels_dict_df)):\n",
    "        label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    return label_codes\n",
    "\n",
    "# preparing label names (just class names)\n",
    "\n",
    "def prepare_label_names(labels_dict, hex_colors_list):\n",
    "    labels_dict_df = prepare_labels(labels_dict, hex_colors_list)   \n",
    "    label_names= list(labels_dict_df.title)\n",
    "    return label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cf9ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preparing label codes (just rgb values in order of classes)\n",
    "\n",
    "def prepare_label_codes(labels_dict, hex_colors_list):\n",
    "    labels_dict_df = prepare_labels(labels_dict, hex_colors_list)\n",
    "    \n",
    "    label_codes = []\n",
    "    r= np.asarray(labels_dict_df.r)\n",
    "    g= np.asarray(labels_dict_df.g)\n",
    "    b= np.asarray(labels_dict_df.b)\n",
    "\n",
    "    for i in range(len(labels_dict_df)):\n",
    "        label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    return label_codes\n",
    "\n",
    "# preparing label names (just class names)\n",
    "\n",
    "def prepare_label_names(labels_dict, hex_colors_list):\n",
    "    labels_dict_df = prepare_labels(labels_dict, hex_colors_list)   \n",
    "    label_names= list(labels_dict_df.title)\n",
    "    return label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662e638",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label_codes = prepare_label_codes(labels_dict, hex_colors_list)\n",
    "label_names = prepare_label_names(labels_dict, hex_colors_list)\n",
    "\n",
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9027b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'label codes are {label_codes}')\n",
    "print('----')\n",
    "print(f'label names are {label_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d5d39",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions to One-hot Encode RGB Labels/Masks and Decoding Encoded Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cae4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoder \n",
    "def rgb_to_onehot(rgb_mask_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 512 x 512 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    rgb_mask_to_encode = rgb_mask_image*255\n",
    "#    print(f'shape of mask to encode is {rgb_mask_to_encode.shape}')\n",
    "    # shape prepared for image size and channels = num of classes (instead of 3 RGB colors)\n",
    "    shape = rgb_mask_to_encode.shape[:2]+(num_classes,)\n",
    "#    print(f'prepared shape for encoded mask is {shape}')\n",
    "    \n",
    "    # encoded_image prepare array with right shaoe \n",
    "    encoded_mask = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        # image.reshape flattens and keeps 3 channels, then checks which pixels same as color in colormap\n",
    "        # then change back to image size for each of 6 channels (based on colormap)\n",
    "        encoded_mask[:,:,i] = np.all(rgb_mask_to_encode.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    \n",
    "#    print(f'encoded mask shape is {encoded_mask.shape}')\n",
    "    return encoded_mask\n",
    "\n",
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)\n",
    "\n",
    "def preprocess_images (images):\n",
    "    images = images/255.\n",
    "    return images\n",
    "\n",
    "def encoding_masks(masks_dataset, colormap = id2code):\n",
    "    encoded_masks = []\n",
    "#    print(f'range is {len(masks_dataset)}')\n",
    "    \n",
    "    for i in range(len(masks_dataset)): \n",
    "        mask = masks_dataset[i]\n",
    "        encoded_mask = rgb_to_onehot(mask, colormap)\n",
    "        encoded_masks.append(encoded_mask)\n",
    "    \n",
    "    encoded_masks = np.array(encoded_masks) \n",
    "    return encoded_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aed2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# checking that it works\n",
    "mask = masks_dataset[5]\n",
    "print(f'mask shape is RGB image {mask.shape}')\n",
    "encoded_mask = rgb_to_onehot(mask, colormap = id2code)\n",
    "decoded_mask = onehot_to_rgb(encoded_mask, colormap = id2code)\n",
    "\n",
    "print(f'encoded mask is 6 channel array {encoded_mask.shape}')\n",
    "print(f'decoded mask is again RGB image {decoded_mask.shape}')\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(6, 6)) \n",
    "ax[0].imshow(mask)\n",
    "ax[1].imshow(decoded_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e36c9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ff244",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loading X and y, split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564c725",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(images_dataset), len(masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5329e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(images_dataset[0:TRIAL_SIZE]), len(masks_dataset[0:TRIAL_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a652",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preparing X(images) and y(labels) \n",
    "y = encoding_masks(masks_dataset[0:TRIAL_SIZE])\n",
    "X = preprocess_images (images_dataset[0:TRIAL_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bb6c9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Finally we shuffle:\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# split is for train/val data and for test data\n",
    "split = int(len(X) /6.) \n",
    "X_test, X_train = X[:split], X[split:]\n",
    "y_test, y_train = y[:split], y[split:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2a684",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(X_test), len(X_train), len(y_test), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007608e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test.shape, X_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e0b45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random check of 6 masks and respective images\n",
    "\n",
    "i = random.randint(0, len(y_train)-5)\n",
    "\n",
    "f, ax = plt.subplots(2, 6, figsize=(15, 5)) \n",
    "\n",
    "ax[0,0].imshow(onehot_to_rgb(y_train[i]))\n",
    "ax[0,1].imshow(onehot_to_rgb(y_train[i+1]))\n",
    "ax[0,2].imshow(onehot_to_rgb(y_train[i+2]))\n",
    "ax[0,3].imshow(onehot_to_rgb(y_train[i+3]))\n",
    "ax[0,4].imshow(onehot_to_rgb(y_train[i+4]))\n",
    "ax[0,5].imshow(onehot_to_rgb(y_train[i+5]))\n",
    "\n",
    "\n",
    "ax[1,0].imshow(X_train[i])\n",
    "ax[1,1].imshow(X_train[i+1])\n",
    "ax[1,2].imshow(X_train[i+2])\n",
    "ax[1,3].imshow(X_train[i+3])\n",
    "ax[1,4].imshow(X_train[i+4])\n",
    "ax[1,5].imshow(X_train[i+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058d5d2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setting up InceptionResNetV2 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa7329",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_inception_resnetv2_unet(input_shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
    "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "\n",
    "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
    "    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n",
    "\n",
    "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
    "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
    "\n",
    "    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n",
    "    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n",
    "    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "    \n",
    "    \"\"\" Output \"\"\"\n",
    "    dropout = Dropout(0.3)(d4)\n",
    "    outputs = Conv2D(11, 1, padding=\"same\", activation=\"softmax\")(dropout)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f77026",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "\n",
    "model = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\n",
    "model.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c9ae2",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key parameters for training the model + savings results\n",
    "PATIENCE = 4\n",
    "BATCH_SIZE = 1 # smaller potentially better for model to perform, but means more epochs\n",
    "\n",
    "steps_per_epoch = np.ceil(float(len(X_train)*0.8) / float(BATCH_SIZE))\n",
    "print('steps_per_epoch: ', steps_per_epoch)\n",
    "\n",
    "validation_steps = np.ceil(float(len(X_train)*0.2) / float(BATCH_SIZE))\n",
    "print('validation_steps: ', validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.0001, 60)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = model_path,\n",
    "    save_best_only = True, \n",
    "#     save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = PATIENCE, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename = results_path,\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split = 0.2, \n",
    "    epochs = 50,\n",
    "    callbacks=callbacks, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6f787",
   "metadata": {},
   "source": [
    "### Loading results and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model doesn't finish, this will not work\n",
    "# then we load it from the cvs result\n",
    "\n",
    "df_result = pd.DataFrame(history.history)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(40, 5))\n",
    "ax = ax.ravel()\n",
    "metrics = ['Dice Coefficient', 'Accuracy', 'Loss', 'Learning Rate']\n",
    "\n",
    "for i, met in enumerate(['dice_coef', 'accuracy', 'loss', 'lr']): \n",
    "    if met != 'lr':\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel(metrics[i])\n",
    "        ax[i].set_xticks(np.arange(0,45,4))\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "    else:\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel(metrics[i])\n",
    "        ax[i].set_xticks(np.arange(0,45,4))\n",
    "        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        \n",
    "plt.savefig(model_metrics_plot_path, facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualized all prediction and save the combo: \n",
    "#    - image, ground truth mask, predicted mask\n",
    "# as a file\n",
    "\n",
    "np.shape(pred_all)\n",
    "\n",
    "count = 0\n",
    " \n",
    "for j in range(0,np.shape(pred_all)[0]):\n",
    "    count += 1\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(batch_img[j])\n",
    "    ax1.set_title('Input Image', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground Truth Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax2.imshow(onehot_to_rgb(batch_mask[j],id2code))\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax3.imshow(onehot_to_rgb(pred_all[j],id2code))\n",
    "    ax3.grid(False)\n",
    "\n",
    "#    plt.savefig('./predictions/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
