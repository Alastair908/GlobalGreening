{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfc6910",
   "metadata": {},
   "source": [
    "# Global Greening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48798",
   "metadata": {},
   "source": [
    "## Installing & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from patchify import patchify\n",
    "import albumentations as A\n",
    "from IPython.display import SVG\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil, cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d1134",
   "metadata": {},
   "source": [
    "## Prepara Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d5c7a",
   "metadata": {},
   "source": [
    "**Augmentation using Albumentations Library**\n",
    "\n",
    "[Albumentations](https://albumentations.ai/) is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n",
    "\n",
    "Data augmentation is done by the following techniques:\n",
    "\n",
    "1. Random Cropping - left out since we will have same size pictures\n",
    "2. Horizontal Flipping\n",
    "3. Vertical Flipping\n",
    "4. Rotation\n",
    "5. Random Brightness & Contrast\n",
    "6. Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "7. Grid Distortion\n",
    "8. Optical Distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to augment\n",
    "def augment(): #width, height\n",
    "    transform = A.Compose([\n",
    "#        A.RandomCrop(width=width, height=height, p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n",
    "        A.OneOf([\n",
    "            A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),\n",
    "            A.GridDistortion(p=0.5),\n",
    "            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n",
    "        ], p=1.0),\n",
    "    ], p=1.0)\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the augmentations\n",
    "\n",
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 16\n",
    "\n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(10, 10)) \n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(16, 12))  \n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n",
    "\n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n",
    "        \n",
    "    plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b841e",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbe364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where we are\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac47efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dataset_root_folder = '/Users/Alenka/code/Alastair908/Downloads'\n",
    "dataset_name = 'Dubai_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading images and masks into the list - new version wihtout cv2.\n",
    "\n",
    "images_dataset = []\n",
    "masks_dataset = []\n",
    "\n",
    "for image_type in ['images' , 'masks']:\n",
    "    for tile_id in range(1,9):\n",
    "        for image_id in range(1,10):                    \n",
    "            if image_type == 'images':\n",
    "                image_extension = 'jpg'\n",
    "                path_image = f'{dataset_root_folder}/{dataset_name}/Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}'\n",
    "                print(path_image)\n",
    "                image = Image.open(path_image)\n",
    "                image = image.resize((512,512))\n",
    "                images_dataset.append(np.array(image))\n",
    "                print('appended image')\n",
    "            elif image_type == 'masks':\n",
    "                image_extension = 'png'\n",
    "                path_mask = f'{dataset_root_folder}/{dataset_name}/Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}'\n",
    "                print(path_mask)\n",
    "                mask = Image.open(path_mask)\n",
    "                mask = mask.resize((512,512)).convert('RGB')\n",
    "                masks_dataset.append(np.array(mask))\n",
    "                print('appended mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_dataset), len(masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac963f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_dataset[0] \n",
    "mask = masks_dataset[0] \n",
    "\n",
    "print(image.shape, mask.shape, type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(6, 6)) \n",
    "ax[0].imshow(images_dataset[0])\n",
    "ax[1].imshow(masks_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bf3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code causing trouble\n",
    "# visualize(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3b92c",
   "metadata": {},
   "source": [
    "## Image masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac7eed",
   "metadata": {},
   "source": [
    "The images are densely labeled and contain the following 6 classes:\n",
    "\n",
    "| Name       | R   | G   | B   | Color                                                                                              |\n",
    "| ---------- | --- | --- | --- | -------------------------------------------------------------------------------------------------- |\n",
    "| Building   | 60  | 16  | 152 | <p align=\"center\"><div style=\"background-color: rgb(60, 16, 152); padding: 10px; \"/></p>   |\n",
    "| Land       | 132 | 41  | 246 | <p align=\"center\"><div style=\"background-color: rgb(132, 41, 246); padding: 10px; \"/></p>   |\n",
    "| Road       | 110 | 193 | 228 | <p align=\"center\"><div style=\"background-color: rgb(110, 193, 228); padding: 10px; \"/></p>   |\n",
    "| Vegetation | 254 | 221 | 58  | <p align=\"center\"><div style=\"background-color: rgb(254, 221, 58); padding: 10px; \"/></p>   |\n",
    "| Water      | 226 | 169 | 41  | <p align=\"center\"><div style=\"background-color: rgb(226, 169, 41); padding: 10px; \"/></p>   |\n",
    "| Unlabeled  | 155 | 155 | 155 | <p align=\"center\"><div style=\"background-color: rgb(155, 155, 155); padding: 10px; \"/></p>   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb79bd",
   "metadata": {},
   "source": [
    "## Perform Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965be2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = augment()\n",
    "transformed = transform(image=image, mask=mask)\n",
    "transformed_image = transformed['image']\n",
    "transformed_mask = transformed['mask']\n",
    "\n",
    "visualize(transformed_image, transformed_mask, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(count):\n",
    "    '''Function for data augmentation\n",
    "        Input:\n",
    "            count - total no. of images after augmentation = initial no. of images * count\n",
    "        Output:\n",
    "            writes augmented images (input images & segmentation masks) to the working directory\n",
    "    '''\n",
    "    transform = augment() \n",
    "    aug_images_dataset = []\n",
    "    aug_masks_dataset = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(count):\n",
    "        for j in range(len(images_dataset)):\n",
    "            img = images_dataset[j]\n",
    "            msk = masks_dataset[j] \n",
    "            \n",
    "            transformed = transform(image=img, mask=msk)\n",
    "            transformed_image = transformed['image']\n",
    "            transformed_mask = transformed['mask'] \n",
    "       \n",
    "            aug_images_dataset.append(transformed_image)\n",
    "            aug_masks_dataset.append(transformed_mask)\n",
    "    return aug_images_dataset, aug_masks_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_images_dataset, aug_masks_dataset  = augment_dataset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_dataset), len(masks_dataset), len(aug_images_dataset), len(aug_masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = []\n",
    "\n",
    "for i in range(len(images_dataset)):\n",
    "    image_sizes.append(images_dataset[i].shape)\n",
    "\n",
    "image_sizes\n",
    "min(image_sizes), max(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sizes = []\n",
    "\n",
    "for i in range(len(masks_dataset)):\n",
    "    mask_sizes.append(masks_dataset[i].shape)\n",
    "\n",
    "mask_sizes\n",
    "min(mask_sizes), max(mask_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 3 augmented images + masks for first image \n",
    "\n",
    "f, ax = plt.subplots(2, 4, figsize=(12, 6)) \n",
    "\n",
    "ax[0,0].imshow(images_dataset[0])\n",
    "ax[0,1].imshow(masks_dataset[0])\n",
    "\n",
    "ax[0,2].imshow(aug_images_dataset[0])\n",
    "ax[0,3].imshow(aug_masks_dataset[0])\n",
    "\n",
    "ax[1,0].imshow(aug_images_dataset[72])\n",
    "ax[1,1].imshow(aug_masks_dataset[72])\n",
    "\n",
    "ax[1,2].imshow(aug_images_dataset[144])\n",
    "ax[1,3].imshow(aug_masks_dataset[144])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94020d40",
   "metadata": {},
   "source": [
    "## Preparing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_dict = {\"classes\": [{\"title\": \"Water\", \"shape\": \"polygon\", \"color\": \"#50E3C2\", \"geometry_config\": {}}, {\"title\": \"Land (unpaved area)\", \"shape\": \"polygon\", \"color\": \"#F5A623\", \"geometry_config\": {}}, {\"title\": \"Road\", \"shape\": \"polygon\", \"color\": \"#DE597F\", \"geometry_config\": {}}, {\"title\": \"Building\", \"shape\": \"polygon\", \"color\": \"#D0021B\", \"geometry_config\": {}}, {\"title\": \"Vegetation\", \"shape\": \"polygon\", \"color\": \"#417505\", \"geometry_config\": {}}, {\"title\": \"Unlabeled\", \"shape\": \"polygon\", \"color\": \"#9B9B9B\", \"geometry_config\": {}}]}\n",
    "labels_dict = {\"classes\": [{\"title\": \"Water\", \"r\": 226, \"g\": 169, \"b\": 41 }, \n",
    "                           {\"title\": \"Land\", \"r\": 132, \"g\": 41, \"b\": 246 }, \n",
    "                           {\"title\": \"Road\", \"r\": 110, \"g\": 193, \"b\": 228 }, \n",
    "                           {\"title\": \"Building\", \"r\": 60, \"g\": 16, \"b\": 152 }, \n",
    "                           {\"title\": \"Vegetation\", \"r\": 254, \"g\": 221, \"b\": 58 }, \n",
    "                           {\"title\": \"Unlabeled\", \"r\": 155, \"g\": 155, \"b\": 155 }]}\n",
    "\n",
    "labels_dict_df = pd.DataFrame(labels_dict['classes'])\n",
    "labels_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names= list(labels_dict_df.title)\n",
    "label_codes = []\n",
    "r= np.asarray(labels_dict_df.r)\n",
    "g= np.asarray(labels_dict_df.g)\n",
    "b= np.asarray(labels_dict_df.b)\n",
    "\n",
    "for i in range(len(labels_dict_df)):\n",
    "    label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    \n",
    "label_codes, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d223777",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d5d39",
   "metadata": {},
   "source": [
    "## Function to One-hot Encode RGB Labels/Masks and Decoding Encoded Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_mask_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    # shape prepared for image size and channels = num of classes (instead of 3 RGB colors)\n",
    "    shape = rgb_mask_image.shape[:2]+(num_classes,)\n",
    "    # encoded_image prepare array with right shaoe \n",
    "    encoded_mask = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        # image.reshape flattens and keeps 3 channels, \n",
    "        # then checks which pixels same as color in colormap\n",
    "        # then change back to image size for each of 6 channels (based on colormap)\n",
    "        encoded_mask[:,:,i] = np.all(rgb_mask_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "\n",
    "    return encoded_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that it works\n",
    "print(f'mask shape is RGB image {mask.shape}')\n",
    "encoded_mask = rgb_to_onehot(mask, colormap = id2code)\n",
    "decoded_mask = onehot_to_rgb(encoded_mask, colormap = id2code)\n",
    "plt.imshow(decoded_mask);\n",
    "print(f'encoded mask is 6 channel array {encoded_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012ddc7",
   "metadata": {},
   "source": [
    "**Input on loading and preprocessing Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df703a95",
   "metadata": {},
   "source": [
    "Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\n",
    "\n",
    "\n",
    "we will use function [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "\n",
    "and resize with [keras.layers.resizing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing)\n",
    "\n",
    "did resizing on import with Pillow.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e36c9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files to use\n",
    "len(images_dataset), len(masks_dataset), len(aug_images_dataset), len(aug_masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd88c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files to use\n",
    "images_dataset[0].shape, masks_dataset[0].shape, aug_images_dataset[0].shape, aug_masks_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce293d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of images and masks\n",
    "image_full_dataset = images_dataset + aug_images_dataset\n",
    "image_full_dataset_np = np.array(image_full_dataset)\n",
    "\n",
    "masks_full_dataset = masks_dataset + aug_masks_dataset\n",
    "#masks_full_dataset_np = np.array(masks_full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ee312",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Problem solving - delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e2aec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_full_dataset = images_dataset + aug_images_dataset\n",
    "len(image_full_dataset), type(image_full_dataset), type(image_full_dataset[0]), image_full_dataset[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4875542",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_full_dataset = masks_dataset + aug_masks_dataset\n",
    "len(masks_full_dataset ), type(masks_full_dataset ), type(masks_full_dataset[0]), masks_full_dataset[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c4152",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_full_dataset_np = np.array(image_full_dataset)\n",
    "len(image_full_dataset_np), type(image_full_dataset_np), type(image_full_dataset_np[0]), image_full_dataset_np[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6ffdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_full_dataset_np = np.array(masks_full_dataset)\n",
    "#len(masks_full_dataset_), type(masks_full_dataset ), type(masks_full_dataset[0]), masks_full_dataset[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e3f24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896ca5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(image_full_dataset), image_full_dataset[0].shape, image_full_dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5f7e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(masks_full_dataset)\n",
    "len(masks_full_dataset), masks_full_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80def43",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e08be7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(masks_full_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb606f6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask1 = masks_dataset[0]\n",
    "mask2 = masks_dataset[10]\n",
    "encoded_mask1 = rgb_to_onehot(mask1)\n",
    "encoded_mask2 = rgb_to_onehot(mask2)\n",
    "mask1.shape, encoded_mask2.shape\n",
    "mask2.shape, encoded_mask2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77d9d3",
   "metadata": {},
   "source": [
    "#### Back to modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_masks = []\n",
    "\n",
    "for i in range(len(masks_full_dataset)):\n",
    "    mask = masks_full_dataset[i]\n",
    "    encoded_mask = rgb_to_onehot(mask)\n",
    "    encoded_masks.append(encoded_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69498ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(encoded_masks) \n",
    "X = np.array(image_full_dataset)/255.\n",
    "len(y), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82970cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be141a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing X(images) and y(labels) - to be added to load images later\n",
    "\n",
    "# Finally we shuffle:\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# first split is for train/val data, second split for test data\n",
    "first_split = int(len(X) /6.) \n",
    "X_test, X_train_val = X[:first_split], X[first_split:]\n",
    "y_test, y_train_val = y[:first_split], y[first_split:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832803e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7058d5d2",
   "metadata": {},
   "source": [
    "### InceptionResNetV2 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_inception_resnetv2_unet(input_shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
    "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "\n",
    "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
    "    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n",
    "\n",
    "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
    "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
    "\n",
    "    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n",
    "    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n",
    "    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "    \n",
    "    \"\"\" Output \"\"\"\n",
    "    dropout = Dropout(0.3)(d4)\n",
    "    outputs = Conv2D(6, 1, padding=\"same\", activation=\"softmax\")(dropout)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f77026",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# 1 is smooth coefficient - the dice_coef as IoU is \n",
    "# 1. the area of overlap between the predicted segmentation and the ground truth \n",
    "# 2. divided by the area of union between the predicted segmentation and the ground truth\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "\n",
    "model = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\n",
    "model.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c9ae2",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.0001, 60)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# save the model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = 'InceptionResNetV2-UNet.h5',\n",
    "    save_best_only = True, \n",
    "#     save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = 12, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"model_training.csv\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4219e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "steps_per_epoch = np.ceil(float(len(X_train_val)*0.8) / float(batch_size))\n",
    "print('steps_per_epoch: ', steps_per_epoch)\n",
    "\n",
    "validation_steps = np.ceil(float(len(X_train_val)*0.2) / float(batch_size))\n",
    "print('validation_steps: ', validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    batch_size=batch_size,\n",
    "    validation_split = 0.2, \n",
    "    epochs = 50,\n",
    "    callbacks=callbacks, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(history.history)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39140fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load history from csv file\n",
    "history_saved = pd.read_csv(\"model_training.csv\")\n",
    "history_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted to show based on history_saved\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
    "ax = ax.ravel()\n",
    "metrics = ['Dice Coefficient', 'Accuracy', 'Loss', 'Learning Rate']\n",
    "\n",
    "for i, met in enumerate(['dice_coef', 'accuracy', 'loss', 'lr']): \n",
    "    if met != 'lr':\n",
    "        ax[i].plot(history_saved[met])\n",
    "        ax[i].plot(history_saved['val_' + met])\n",
    "        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel(metrics[i])\n",
    "        ax[i].set_xticks(np.arange(0,45,4))\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "#     else:\n",
    "#         ax[i].plot(history_saved[met])\n",
    "#         ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "#         ax[i].set_xlabel('Epochs')\n",
    "#         ax[i].set_ylabel(metrics[i])\n",
    "#         ax[i].set_xticks(np.arange(0,45,4))\n",
    "#         ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "#         ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        \n",
    "plt.savefig('model_metrics_plot.png', facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this portion not working as we dont have history\n",
    "# fig, ax = plt.subplots(1, 4, figsize=(40, 5))\n",
    "# ax = ax.ravel()\n",
    "# metrics = ['Dice Coefficient', 'Accuracy', 'Loss', 'Learning Rate']\n",
    "\n",
    "# for i, met in enumerate(['dice_coef', 'accuracy', 'loss', 'lr']): \n",
    "#     if met != 'lr':\n",
    "#         ax[i].plot(history.history[met])\n",
    "#         ax[i].plot(history.history['val_' + met])\n",
    "#         ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "#         ax[i].set_xlabel('Epochs')\n",
    "#         ax[i].set_ylabel(metrics[i])\n",
    "#         ax[i].set_xticks(np.arange(0,45,4))\n",
    "#         ax[i].legend(['Train', 'Validation'])\n",
    "#         ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "#         ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "#     else:\n",
    "#         ax[i].plot(history.history[met])\n",
    "#         ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n",
    "#         ax[i].set_xlabel('Epochs')\n",
    "#         ax[i].set_ylabel(metrics[i])\n",
    "#         ax[i].set_xticks(np.arange(0,45,4))\n",
    "#         ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "#         ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n",
    "        \n",
    "# plt.savefig('model_metrics_plot.png', facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./InceptionResNetV2-UNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30071f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefab128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_all= model.predict(X_test)\n",
    "np.shape(pred_all)\n",
    "count = 0\n",
    "\n",
    "for j in range(0,np.shape(pred_all)[0]):\n",
    "    count += 1\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(X_test[j])\n",
    "    ax1.set_title('Input Image', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground Truth Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax2.imshow(onehot_to_rgb(y_test[j],id2code))\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax3.imshow(onehot_to_rgb(pred_all[j],id2code))\n",
    "    ax3.grid(False)\n",
    "\n",
    "    plt.savefig('./predictions/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f6622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642ef0fb",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccce06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Resizing(\n",
    "    height,\n",
    "    width,\n",
    "    interpolation='bilinear',\n",
    "    crop_to_aspect_ratio=False,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "preprocessing = \n",
    "tf.keras.layers.CenterCrop(\n",
    "    512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "  print(element)\n",
    "\n",
    "tf.keras.preprocessing.image.smart_resize(\n",
    "    x, size, interpolation='bilinear'\n",
    ")\n",
    "\n",
    "size = (512, 512)\n",
    "ds = ds.map(lambda img: tf.image.resize(img, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f76b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d82eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccd62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
