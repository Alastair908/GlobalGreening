{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfc6910",
   "metadata": {},
   "source": [
    "# Global Greening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48798",
   "metadata": {},
   "source": [
    "## Installing & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605ac69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/Alenka/.pyenv/versions/3.10.6/envs/GlobalGreening/lib/python3.10/site-packages/tensorflow_io-0.32.0-py3.10-macosx-13.3-arm64.egg/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from patchify import patchify\n",
    "import albumentations as A\n",
    "from IPython.display import SVG\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil, cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model, image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fab3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2af3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/kunnalparihar/Satellite-Image-Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091f3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b70d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919d1134",
   "metadata": {},
   "source": [
    "## Prepara Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d5c7a",
   "metadata": {},
   "source": [
    "**Augmentation using Albumentations Library**\n",
    "\n",
    "[Albumentations](https://albumentations.ai/) is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n",
    "\n",
    "Data augmentation is done by the following techniques:\n",
    "\n",
    "1. Random Cropping - left out since we will have same size pictures\n",
    "2. Horizontal Flipping\n",
    "3. Vertical Flipping\n",
    "4. Rotation\n",
    "5. Random Brightness & Contrast\n",
    "6. Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "7. Grid Distortion\n",
    "8. Optical Distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8185561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to augment\n",
    "def augment(): #width, height\n",
    "    transform = A.Compose([\n",
    "#        A.RandomCrop(width=width, height=height, p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n",
    "        A.OneOf([\n",
    "            A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),\n",
    "            A.GridDistortion(p=0.5),\n",
    "            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n",
    "        ], p=1.0),\n",
    "    ], p=1.0)\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the augmentations\n",
    "\n",
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 16\n",
    "\n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(10, 10)) \n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(16, 12))  \n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n",
    "\n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n",
    "        \n",
    "    plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b841e",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adbe364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Alenka/code/Alastair908/GlobalGreening\r\n"
     ]
    }
   ],
   "source": [
    "# check where we are\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac47efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dataset_root_folder = '/Users/Alenka/OneDrive/GlobalGreening'\n",
    "dataset_name = 'zoomed_photos_model_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898089f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_batch_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f3f865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating file names from the directory\n",
    "\n",
    "images_dir = f'{dataset_root_folder}/{dataset_name}'\n",
    "file_names = np.sort(os.listdir(images_dir)) \n",
    "file_names = np.char.rstrip(file_names, '.png')\n",
    "file_names = np.char.split(file_names, '_') \n",
    "\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171ae05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating df with information about longitude, latitude (can also be used to load the images)\n",
    "\n",
    "image_geo_locations = np.zeros((len(file_names),2))\n",
    "image_geo_locations = pd.DataFrame(image_geo_locations, columns=['latitude', 'longitude'])\n",
    "\n",
    "for image_type in ['latitude', 'longitude']:\n",
    "    for i in range(len(file_names)):\n",
    "        file = file_names[i]\n",
    "        \n",
    "        if image_type == 'latitude':\n",
    "            text = file[0]\n",
    "            image_number = ''.join(num for num in text if num.isdigit())\n",
    "            latitude = file[1].strip('-') \n",
    "#            print(f'latitude is {latitude}')\n",
    "            image_geo_locations.at[int(image_number),'latitude'] = latitude\n",
    "                                   \n",
    "        elif image_type == 'longitude':\n",
    "            text = file[0]\n",
    "            image_number = ''.join(num for num in text if num.isdigit())\n",
    "            longitude = file[2] \n",
    "#            print(f'longitude is {longitude}')\n",
    "            image_geo_locations.at[int(image_number),'longitude'] = longitude                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4065a05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109.0</td>\n",
       "      <td>37.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>108.96</td>\n",
       "      <td>39.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>108.96</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>108.96</td>\n",
       "      <td>40.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>108.96</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>108.96</td>\n",
       "      <td>40.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude longitude\n",
       "0      109.0      37.0\n",
       "1      109.0     37.04\n",
       "2      109.0     37.07\n",
       "3      109.0     37.11\n",
       "4      109.0     37.14\n",
       "..       ...       ...\n",
       "195   108.96     39.99\n",
       "196   108.96     40.02\n",
       "197   108.96     40.06\n",
       "198   108.96      40.1\n",
       "199   108.96     40.13\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_geo_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading images and masks into the list - new version wihtout cv2.\n",
    "\n",
    "images_dataset = []\n",
    "masks_dataset = []\n",
    "\n",
    "for image_type in ['images' , 'masks', 'latitude', 'longitude']:\n",
    "    for i in range(batch_size):\n",
    "        if image_type == 'images':\n",
    "        if image_type == 'images':\n",
    "        \n",
    "        if image_type == 'images':\n",
    "            image_extension = 'png'\n",
    "            path_image = f'{dataset_root_folder}/{dataset_name}/{image_type}/image_part_00{image_id}.{image_extension}'\n",
    "            print(path_image)\n",
    "            image = Image.open(path_image)\n",
    "            images_dataset.append(np.array(image))\n",
    "            print('appended image')\n",
    "        elif image_type == 'masks':\n",
    "                image_extension = 'png'\n",
    "                path_mask = f'{dataset_root_folder}/{dataset_name}/Tile {tile_id}/{image_type}/image_part_00{image_id}.{image_extension}'\n",
    "                print(path_mask)\n",
    "                mask = Image.open(path_mask)\n",
    "                masks_dataset.append(np.array(mask))\n",
    "                print('appended mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66388851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how are files loaded into Dataset\n",
    "\n",
    "file_names_unsorted = os.listdir(images_dir)\n",
    "print(len(file_names_unsorted))\n",
    "file_names_unsorted\n",
    "\n",
    "image_file_names = os.walk(f'{dataset_root_folder}/{dataset_name}')\n",
    "for image in image_file_names:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_tf_dataset = image_dataset_from_directory(\n",
    "    directory = f'{dataset_root_folder}/{dataset_name}',\n",
    "    labels=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=200,\n",
    "    image_size=(512, 512),\n",
    "    shuffle=True,\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_single_element randomly chooses an element\n",
    "\n",
    "plt.imshow(images_tf_dataset.get_single_element()[0]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_dataset), len(masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac963f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_dataset[0] \n",
    "mask = masks_dataset[0] \n",
    "\n",
    "print(image.shape, mask.shape, type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(6, 6)) \n",
    "ax[0].imshow(images_dataset[0])\n",
    "ax[1].imshow(masks_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bf3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3b92c",
   "metadata": {},
   "source": [
    "## Image masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac7eed",
   "metadata": {},
   "source": [
    "The images are densely labeled and contain the following 6 classes:\n",
    "\n",
    "| Name       | R   | G   | B   | Color                                                                                              |\n",
    "| ---------- | --- | --- | --- | -------------------------------------------------------------------------------------------------- |\n",
    "| Building   | 60  | 16  | 152 | <p align=\"center\"><div style=\"background-color: rgb(60, 16, 152); padding: 10px; \"/></p>   |\n",
    "| Land       | 132 | 41  | 246 | <p align=\"center\"><div style=\"background-color: rgb(132, 41, 246); padding: 10px; \"/></p>   |\n",
    "| Road       | 110 | 193 | 228 | <p align=\"center\"><div style=\"background-color: rgb(110, 193, 228); padding: 10px; \"/></p>   |\n",
    "| Vegetation | 254 | 221 | 58  | <p align=\"center\"><div style=\"background-color: rgb(254, 221, 58); padding: 10px; \"/></p>   |\n",
    "| Water      | 226 | 169 | 41  | <p align=\"center\"><div style=\"background-color: rgb(226, 169, 41); padding: 10px; \"/></p>   |\n",
    "| Unlabeled  | 155 | 155 | 155 | <p align=\"center\"><div style=\"background-color: rgb(155, 155, 155); padding: 10px; \"/></p>   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb79bd",
   "metadata": {},
   "source": [
    "## Perform Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965be2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = augment()\n",
    "transformed = transform(image=image, mask=mask)\n",
    "transformed_image = transformed['image']\n",
    "transformed_mask = transformed['mask']\n",
    "\n",
    "visualize(transformed_image, transformed_mask, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(count):\n",
    "    '''Function for data augmentation\n",
    "        Input:\n",
    "            count - total no. of images after augmentation = initial no. of images * count\n",
    "        Output:\n",
    "            writes augmented images (input images & segmentation masks) to the working directory\n",
    "    '''\n",
    "    transform = augment() \n",
    "    aug_images_dataset = []\n",
    "    aug_masks_dataset = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(count):\n",
    "        for j in range(len(images_dataset)):\n",
    "            img = images_dataset[j]\n",
    "            msk = masks_dataset[j] \n",
    "            \n",
    "            transformed = transform(image=img, mask=msk)\n",
    "            transformed_image = transformed['image']\n",
    "            transformed_mask = transformed['mask'] \n",
    "       \n",
    "            aug_images_dataset.append(transformed_image)\n",
    "            aug_masks_dataset.append(transformed_mask)\n",
    "    return aug_images_dataset, aug_masks_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_images_dataset, aug_masks_dataset  = augment_dataset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_dataset), len(masks_dataset), len(aug_images_dataset), len(aug_masks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ff97f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image_sizes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mimages_dataset\u001b[49m)):\n\u001b[1;32m      4\u001b[0m     image_sizes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(images_dataset[i]))\n\u001b[1;32m      6\u001b[0m image_sizes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "image_sizes = []\n",
    "\n",
    "for i in range(len(images_dataset)):\n",
    "    image_sizes.append(len(images_dataset[i]))\n",
    "\n",
    "image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sizes = []\n",
    "\n",
    "for i in range(len(masks_dataset)):\n",
    "    mask_sizes.append(len(masks_dataset[i]))\n",
    "\n",
    "min(mask_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009476d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 3 augmented images + masks for first image \n",
    "\n",
    "f, ax = plt.subplots(2, 4, figsize=(12, 6)) \n",
    "\n",
    "ax[0,0].imshow(images_dataset[0])\n",
    "ax[0,1].imshow(masks_dataset[0])\n",
    "\n",
    "ax[0,2].imshow(aug_images_dataset[0])\n",
    "ax[0,3].imshow(aug_masks_dataset[0])\n",
    "\n",
    "ax[1,0].imshow(aug_images_dataset[72])\n",
    "ax[1,1].imshow(aug_masks_dataset[72])\n",
    "\n",
    "ax[1,2].imshow(aug_images_dataset[144])\n",
    "ax[1,3].imshow(aug_masks_dataset[144])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94020d40",
   "metadata": {},
   "source": [
    "## Preparing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_dict = {\"classes\": [{\"title\": \"Water\", \"shape\": \"polygon\", \"color\": \"#50E3C2\", \"geometry_config\": {}}, {\"title\": \"Land (unpaved area)\", \"shape\": \"polygon\", \"color\": \"#F5A623\", \"geometry_config\": {}}, {\"title\": \"Road\", \"shape\": \"polygon\", \"color\": \"#DE597F\", \"geometry_config\": {}}, {\"title\": \"Building\", \"shape\": \"polygon\", \"color\": \"#D0021B\", \"geometry_config\": {}}, {\"title\": \"Vegetation\", \"shape\": \"polygon\", \"color\": \"#417505\", \"geometry_config\": {}}, {\"title\": \"Unlabeled\", \"shape\": \"polygon\", \"color\": \"#9B9B9B\", \"geometry_config\": {}}]}\n",
    "labels_dict = {\"classes\": [{\"title\": \"Water\", \"r\": 226, \"g\": 169, \"b\": 41 }, \n",
    "                           {\"title\": \"Land\", \"r\": 132, \"g\": 41, \"b\": 246 }, \n",
    "                           {\"title\": \"Road\", \"r\": 110, \"g\": 193, \"b\": 228 }, \n",
    "                           {\"title\": \"Building\", \"r\": 60, \"g\": 16, \"b\": 152 }, \n",
    "                           {\"title\": \"Vegetation\", \"r\": 254, \"g\": 221, \"b\": 58 }, \n",
    "                           {\"title\": \"Unlabeled\", \"r\": 155, \"g\": 155, \"b\": 155 }]}\n",
    "\n",
    "labels_dict_df = pd.DataFrame(labels_dict['classes'])\n",
    "labels_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names= list(labels_dict_df.title)\n",
    "label_codes = []\n",
    "r= np.asarray(labels_dict_df.r)\n",
    "g= np.asarray(labels_dict_df.g)\n",
    "b= np.asarray(labels_dict_df.b)\n",
    "\n",
    "for i in range(len(labels_dict_df)):\n",
    "    label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    \n",
    "label_codes, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d223777",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d5d39",
   "metadata": {},
   "source": [
    "## Function to One-hot Encode RGB Labels/Masks and Decoding Encoded Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_mask_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    # shape prepared for image size and channels = num of classes (instead of 3 RGB colors)\n",
    "    shape = rgb_mask_image.shape[:2]+(num_classes,)\n",
    "    # encoded_image prepare array with right shaoe \n",
    "    encoded_mask = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        # image.reshape flattens and keeps 3 channels, \n",
    "        # then checks which pixels same as color in colormap\n",
    "        # then change back to image size for each of 6 channels (based on colormap)\n",
    "        encoded_mask[:,:,i] = np.all(rgb_mask_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "\n",
    "    return encoded_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that it works\n",
    "print(f'mask shape is RGB image {mask.shape}')\n",
    "encoded_mask = rgb_to_onehot(mask, colormap = id2code)\n",
    "decoded_mask = onehot_to_rgb(encoded_mask, colormap = id2code)\n",
    "plt.imshow(decoded_mask);\n",
    "print(f'encoded mask is 6 channel array {encoded_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012ddc7",
   "metadata": {},
   "source": [
    "**Input on loading and preprocessing Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df703a95",
   "metadata": {},
   "source": [
    "Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\n",
    "\n",
    "\n",
    "we will use function [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "\n",
    "and resize with [keras.layers.resizing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e36c9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "num_train_samples = len(np.sort(os.listdir(train_images+'train')))\n",
    "num_val_samples = len(np.sort(os.listdir(val_images+'val')))\n",
    "\n",
    "steps_per_epoch = np.ceil(float(num_train_samples) / float(batch_size))\n",
    "\n",
    "print('steps_per_epoch: ', steps_per_epoch)\n",
    "\n",
    "validation_steps = np.ceil(float(4 * num_val_samples) / float(batch_size))\n",
    "\n",
    "print('validation_steps: ', validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058d5d2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### InceptionResNetV2 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa7329",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_inception_resnetv2_unet(input_shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
    "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
    "\n",
    "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
    "    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n",
    "\n",
    "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
    "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
    "\n",
    "    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n",
    "    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n",
    "    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "    \n",
    "    \"\"\" Output \"\"\"\n",
    "    dropout = Dropout(0.3)(d4)\n",
    "    outputs = Conv2D(6, 1, padding=\"same\", activation=\"softmax\")(dropout)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f77026",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "\n",
    "model = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\n",
    "model.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1190d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# graph the model\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c9ae2",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.0001, 60)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = 'InceptionResNetV2-UNet.h5',\n",
    "    save_best_only = True, \n",
    "#     save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = 12, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"model_training.csv\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    XXXXXX \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data =  , \n",
    "\n",
    "    epochs = 50,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776ebaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7fa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526c8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefab128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f6622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b59723d0",
   "metadata": {},
   "source": [
    "### This is code copied from the challenge \n",
    "\n",
    "\n",
    "````\n",
    "X = np.array(imgs)\n",
    "num_classes = len(set(labels))\n",
    "y = to_categorical(labels, num_classes)\n",
    "\n",
    "##### Finally we shuffle:\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "first_split = int(len(imgs) /6.)\n",
    "second_split = first_split + int(len(imgs) * 0.2)\n",
    "X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
    "y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
    "    \n",
    "return X_train, y_train, X_val, y_val, X_test, y_test, num_classes\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe586eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccce06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
